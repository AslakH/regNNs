# Regularization of Neural Networks
Project: Exploring soft and hard constraints in artificial neural networks.

The folders *Start training*, *UMI*, *UMI RNN* and *JAX* contains testing of regularization on a simple toy problem, and learning how to use pytorch and build/train NNs.

The folder *Regularization benchmarking* contains the actual project with comparison of regularization techniques. I use the network LeNet, and the datasets MNIST, CIFAR10 and CIFAR100 to do the testing. I test how the regularization techniques perform on test data, and visualize parts of the NN to investigate the effect of regularization.

## Overview of Regularization Methods
(Explanations from ChatGPT)

#### Comparison: No Regularization:
In the absence of any regularization, a model simply minimizes the loss function on the training data. This can lead to overfitting, especially in high-dimensional models with many parameters, because the model becomes too complex and learns to fit the training data too closely, including its noise. As a result, it often performs poorly on unseen data. Regularization methods are therefore used to prevent overfitting by adding constraints to the learning process.  

**L1 Regularization**: L1 regularization, also known as Lasso regularization, involves adding a term to the loss function that penalizes the absolute value of the weights. This encourages the model to have sparse weights, meaning that many weights are zero. This can lead to a model that is easier to interpret, because it effectively performs feature selection, choosing a subset of the input features to focus on.  
**L2 Regularization**: L2 regularization, also known as Ridge regularization, involves adding a term to the loss function that penalizes the square of the weights. This encourages the model to have small weights but does not encourage sparsity. L2 regularization can help prevent overfitting by discouraging the model from relying too much on any single input feature.  
**Elastic Net Regularization**: Elastic Net regularization is a compromise between L1 and L2 regularization. It involves adding a term to the loss function that is a mix of an L1 penalty and an L2 penalty. This allows the model to have some level of sparsity, like L1 regularization, while also encouraging small weights, like L2 regularization.  
**Soft SVB Regularization**: Soft SVB regularization, introduced by Jia et al. 2019, penalizes the model based on the Frobenius norm of the difference between the weights' Gram matrix and the identity matrix. This encourages the model's weights to be orthogonal, which can improve generalization.  
**Hard SVB Regularization**: Hard SVB regularization, similar to Soft SVB, also encourages the model's weights to be orthogonal, but it does so in a more strict manner. It uses a hard constraint instead of a soft penalty, meaning that the model's weights are forced to be orthogonal.  
**Jacobi Regularization**: Jacobi regularization involves adding a term to the loss function that penalizes the Frobenius norm of the squared Jacobian of the model's outputs with respect to its inputs. This encourages the model to have constant output across small changes in the input, which can prevent overfitting by discouraging the model from fitting the training data too closely.  
**Jacobi Determinant Regularization**: Jacobi Determinant regularization involves adding a term to the loss function that penalizes the squared difference between the determinant of the Jacobian of the model's outputs with respect to its inputs and one. This encourages the model to maintain a constant volume change across small changes in the input, which can help prevent overfitting.  
**Dropout Regularization**: Dropout is a form of regularization where, during training, some fraction of the neurons in a layer are randomly "dropped out", or temporarily removed from the network. This has the effect of making the model more robust, because it learns to make predictions without relying on any single neuron.  
**Confidence Penalty Regularization**: Confidence Penalty regularization adds a term to the loss function that penalizes the model for being too confident in its predictions. This is achieved by adding the entropy of the model's predictions to the loss. This encourages the model to make more uncertain predictions, which can help prevent overfitting.  
**Label Smoothing Regularization**: Label Smoothing is a form of regularization where the target labels for a classification problem are replaced with smoothed versions. Instead of having a hard one-hot encoded target, each target will have a small value for each incorrect class and the rest of the value for the correct class. This encourages the model to be less confident, reducing the risk of overfitting and improving generalization.  
**Hessian Regularization**: Hessian regularization involves adding a term to the loss function that penalizes the Frobenius norm of the Hessian of the model's outputs with respect to its inputs. The Hessian is a matrix that describes the second-order derivatives of the model. Penalizing the norm of the Hessian encourages the model to have outputs that change linearly or sub-linearly with respect to small changes in the inputs. This can help prevent overfitting by discouraging the model from fitting the training data too closely.  
**Noise Injection to Inputs**: Noise injection to inputs is a regularization technique where random noise is added to the inputs during training. This encourages the model to be robust to small changes in the inputs. This form of regularization can help prevent overfitting by discouraging the model from fitting the noise in the training data, instead focusing on the underlying patterns that are consistent even when noise is added.  
**Noise Injection to Weights**: Similar to noise injection to inputs, noise injection to weights involves adding random noise to the weights during training. This can be seen as a form of stochastic regularization, as it adds a source of randomness that the model needs to be robust to. By preventing the weights from settling into a fixed value, it encourages the model to explore different parts of the weight space, which can help it avoid overfitting to the training data.  


### Overview of Visualization Techniques
